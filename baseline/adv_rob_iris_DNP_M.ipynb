{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from time import localtime, strftime\n",
    "\n",
    "from sklearn import datasets\n",
    "from snntorch import spikegen\n",
    "from snntorch import functional as SF\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from z3 import *\n",
    "from collections import defaultdict\n",
    "from utils import *\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=f\"log/{cfg.log_name}_{strftime('%m%d_%H-%M-%S', localtime())}.log\", level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def compare(x, y):\n",
    "    xx, yy = int(x.name().split('_')[-1]), int(y.name().split('_')[-1])\n",
    "    return xx-yy\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "iris_data = iris.data / iris.data.max(axis=0)\n",
    "iris_targets = iris.target\n",
    "\n",
    "if shuffle:\n",
    "    assert len(iris_data) == len(iris_data)\n",
    "    perm = np.random.permutation(len(iris_data))\n",
    "    iris_data, iris_targets = iris_data[perm], iris_targets[perm]\n",
    "\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "if train:\n",
    "    net = Net()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "    #loss = nn.CrossEntropyLoss()\n",
    "    loss = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_counter = 0\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for number in range(len(iris_targets)):\n",
    "            data = torch.tensor(iris_data[number], dtype=torch.float)\n",
    "            #targets = torch.tensor([0 if i != iris_targets[number] else 1 for i in range(max(iris_targets)+1)],dtype=torch.float)\n",
    "            targets = torch.tensor([iris_targets[number]])\n",
    "\n",
    "            # make spike trains\n",
    "            data_spike = spikegen.rate(data, num_steps=num_steps) # type: ignore\n",
    "\n",
    "            # forward pass\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data_spike.view(num_steps, -1))\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = torch.zeros((1), dtype=torch.float)\n",
    "            for step in range(num_steps):\n",
    "                loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            if counter % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "            counter += 1\n",
    "            iter_counter += 1\n",
    "    # print(\"Saving model.pth\")\n",
    "    logger.info(\"Saving model.pth\")\n",
    "    torch.save(net, file_name)\n",
    "else:\n",
    "    net = torch.load(file_name)\n",
    "    # print(\"Model loaded\")\n",
    "    logger.info(\"Model loaded\")\n",
    "\n",
    "check = True\n",
    "if check:\n",
    "    acc = 0\n",
    "    perm = np.random.permutation(len(iris_data))\n",
    "    test_data, test_targets = torch.tensor(iris_data[perm][:100], dtype=torch.float), torch.tensor(iris_targets[perm][:100])\n",
    "    for i, data in enumerate(test_data):\n",
    "        spike_data = spikegen.rate(data, num_steps=num_steps) # type: ignore\n",
    "        spk_rec, mem_rec = net(spike_data.view(num_steps, -1))\n",
    "        idx = np.argmax(spk_rec.sum(dim=0).detach().numpy())\n",
    "        if idx == test_targets[i]:\n",
    "            #print(f'match for {test_targets[i]}')\n",
    "            acc += 1\n",
    "        else:\n",
    "            #print(f'Not match for {test_targets[i]}')\n",
    "            pass\n",
    "    # print(f'Accuracy of the model : {acc}%')\n",
    "    logger.info(f'Accuracy of the model : {acc}%')\n",
    "\n",
    "# print()\n",
    "logger.info(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1 \\le t \\le num\\_steps$$\n",
    "$$0 \\le j \\le num\\_layers-1$$\n",
    "$$0 \\le i \\le num\\_nodes_j-1$$\n",
    "$$spike_indicators[(idx_{node}, idx_{layer}, t)]$$\n",
    "$$weights[(from,to,prev layer)]:=[node(from, prev layer) \\to node(to, prev layer+1)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMT encoding\n",
    "\n",
    "# take a random input and make it into a spike train\n",
    "spike_indicators = gen_s_indicator()\n",
    "potentials = gen_p_indicator()\n",
    "weights = gen_w_indicator([w1:=net.fc1.weight, w2:=net.fc2.weight])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon_0(i,0) \\triangleq (P_{i,0}=0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================\n",
    "# Potential Initializations\n",
    "pot_init = gen_initial_potential_term(potentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, S means:\n",
    "$$\\epsilon_1(i,t) \\triangleq \\left(S_{i,t}=\\sum_{j\\in inSynapse(N_i)}x_{j,t}\\cdot w_{j,i}\\right)$$\n",
    "but in this code, it has different meaning:\n",
    "$$S = P_{i,j,t-1} + \\sum_{i\\in Layer_{j-1}}x_{i,j-1,t}\\cdot w_{j-1,i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Inputs\n",
    "'''\n",
    "assign = []\n",
    "for i, spikes_t in enumerate(sample_spike):\n",
    "    for j, spike in enumerate(spikes_t):\n",
    "        if spike == 1:\n",
    "            assign.append(spike_indicators[(j, 0, i+1)])\n",
    "        else:\n",
    "            assign.append(Not(spike_indicators[(j, 0, i + 1)]))\n",
    "'''\n",
    "assign:List[BoolRef] = []\n",
    "\n",
    "# Node eqn\n",
    "node_eqn:List[BoolRef] = []\n",
    "if cfg.use_DNP:\n",
    "    node_eqn += gen_DNP(weights, spike_indicators)\n",
    "node_eqn += gen_node_eqn(weights, spike_indicators, potentials)       \n",
    "\n",
    "#S.push()\n",
    "#print(\"Equations Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = iris_data[np.random.choice(range(len(iris_data)), num_samples)]\n",
    "# print(samples)\n",
    "logger.info(samples)\n",
    "\n",
    "delta_v = {d: 0 for d in deltas}\n",
    "\n",
    "for delta in deltas:\n",
    "    avt = 0\n",
    "    for sample_no, sample in enumerate(samples):\n",
    "        sample_spike = spikegen.rate(torch.tensor(sample, dtype=torch.float), num_steps=num_steps)\n",
    "\n",
    "        spk_rec, mem_rec = net(sample_spike.view(num_steps, -1)) # epsilon 1~5\n",
    "        label = int(spk_rec.sum(dim=0).argmax())\n",
    "\n",
    "        S = Solver()\n",
    "        S.add(assign+node_eqn+pot_init)\n",
    "        # S.add(node_eqn + pot_init)\n",
    "\n",
    "        sum_val = []\n",
    "        prop = []\n",
    "        reuse_flag = True\n",
    "        for timestep, spike_train in enumerate(sample_spike):\n",
    "            #Variables to calculate the total perturbation.\n",
    "            for i, spike in enumerate(spike_train.view(num_input)):\n",
    "                if spike == 1:\n",
    "                    sum_val.append(If(spike_indicators[(i, 0, timestep + 1)], 0.0, 1.0))\n",
    "                else:\n",
    "                    sum_val.append(If(spike_indicators[(i, 0, timestep + 1)], 1.0, 0.0))\n",
    "                #Flip flag if there is any perturbation\n",
    "                reuse_flag = And(\n",
    "                    reuse_flag,\n",
    "                    spike_indicators[(i, 0, timestep + 1)]==spike.bool().item()\n",
    "                    )\n",
    "            \n",
    "            #If Accumulation of Delta until current timestep is 0, reuse y_hat of non-perturbated spike.\n",
    "            if cfg.reuse_level != 0:\n",
    "                _reuse_targets = []\n",
    "                for _out_node in range(layers[2]):\n",
    "                    _reuse_targets.append(\n",
    "                        spike_indicators[(_out_node, 2, timestep+1)]\\\n",
    "                            == spk_rec[timestep, _out_node].bool().item()\n",
    "                        )\n",
    "                    if cfg.reuse_level == 1: continue\n",
    "                    _reuse_targets.append(\n",
    "                        potentials[(_out_node, 2, timestep+1)]\\\n",
    "                            == mem_rec[timestep, _out_node].item()\n",
    "                    )\n",
    "                prop.append(\n",
    "                    Implies(\n",
    "                        reuse_flag,\n",
    "                        And(_reuse_targets)\n",
    "                        )\n",
    "                    )\n",
    "                \n",
    "        prop.append(sum(sum_val) <= delta)\n",
    "        S.add(prop)\n",
    "\n",
    "        # Output property\n",
    "        #tx = time.time()\n",
    "        op = []\n",
    "        intend_sum = sum([2 * spike_indicators[(label, 2, timestep + 1)] for timestep in range(num_steps)])\n",
    "        for t in range(num_output):\n",
    "            if t != op:\n",
    "                op.append(\n",
    "                    Not(intend_sum > sum([2 * spike_indicators[(t, 2, timestep + 1)] for timestep in range(num_steps)]))\n",
    "                )\n",
    "        #print(f'Output Property Done in {time.time() - tx} sec')\n",
    "        \n",
    "        S.add(op)\n",
    "        tx = time.time()\n",
    "        res = S.check()\n",
    "        if str(res) == 'unsat':\n",
    "            delta_v[delta] += 1\n",
    "        del S\n",
    "        tss = time.time()-tx\n",
    "        # print(f'Completed for delta = {delta}, sample = {sample_no} in {tss} sec as {res}')\n",
    "        logger.info(f'Completed for delta = {delta}, sample = {sample_no} in {tss} sec as {res}')\n",
    "        avt = (avt*sample_no + tss)/(sample_no+1)\n",
    "    # print(f'Completed for delta = {delta} with {delta_v[delta]} in avg time {avt} sec')\n",
    "    logger.info(f'Completed for delta = {delta} with {delta_v[delta]} in avg time {avt} sec')\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
