{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import time\n",
    "from time import localtime, strftime\n",
    "\n",
    "from sklearn import datasets\n",
    "from snntorch import spikegen\n",
    "from snntorch import functional as SF\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "\n",
    "from z3 import *\n",
    "from collections import defaultdict\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=f\"dead_neuron_pruning_{strftime('%m%d_%H-%M-%S', localtime())}.log\", level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "shuffle = True\n",
    "beta = 0.95\n",
    "num_steps = 25\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "train = False\n",
    "file_name = 'model_iris.pth'\n",
    "\n",
    "\n",
    "def compare(x, y):\n",
    "    xx, yy = int(x.name().split('_')[-1]), int(y.name().split('_')[-1])\n",
    "    return xx-yy\n",
    "\n",
    "\n",
    "num_input = 4\n",
    "num_hidden = 5\n",
    "num_output = 3\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_input, num_hidden, bias=False)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_output, bias=False)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "iris_data = iris.data / iris.data.max(axis=0)\n",
    "iris_targets = iris.target\n",
    "\n",
    "if shuffle:\n",
    "    assert len(iris_data) == len(iris_data)\n",
    "    perm = np.random.permutation(len(iris_data))\n",
    "    iris_data, iris_targets = iris_data[perm], iris_targets[perm]\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "loss_hist = []\n",
    "test_loss_hist = []\n",
    "counter = 0\n",
    "\n",
    "if train:\n",
    "    net = Net()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "    #loss = nn.CrossEntropyLoss()\n",
    "    loss = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_counter = 0\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for number in range(len(iris_targets)):\n",
    "            data = torch.tensor(iris_data[number], dtype=torch.float)\n",
    "            #targets = torch.tensor([0 if i != iris_targets[number] else 1 for i in range(max(iris_targets)+1)],dtype=torch.float)\n",
    "            targets = torch.tensor([iris_targets[number]])\n",
    "\n",
    "            # make spike trains\n",
    "            data_spike = spikegen.rate(data, num_steps=num_steps)\n",
    "\n",
    "            # forward pass\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data_spike.view(num_steps, -1))\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = torch.zeros((1), dtype=torch.float)\n",
    "            for step in range(num_steps):\n",
    "                loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            if counter % 20 == 0:\n",
    "                print(f\"Epoch {epoch}, Iteration {iter_counter}\")\n",
    "            counter += 1\n",
    "            iter_counter += 1\n",
    "    # print(\"Saving model.pth\")\n",
    "    logger.info(\"Saving model.pth\")\n",
    "    torch.save(net, file_name)\n",
    "else:\n",
    "    net = torch.load(file_name)\n",
    "    # print(\"Model loaded\")\n",
    "    logger.info(\"Model loaded\")\n",
    "\n",
    "check = True\n",
    "if check:\n",
    "    acc = 0\n",
    "    perm = np.random.permutation(len(iris_data))\n",
    "    test_data, test_targets = torch.tensor(iris_data[perm][:100], dtype=torch.float), torch.tensor(iris_targets[perm][:100])\n",
    "    for i, data in enumerate(test_data):\n",
    "        spike_data = spikegen.rate(data, num_steps=num_steps)\n",
    "        spk_rec, mem_rec = net(spike_data.view(num_steps, -1))\n",
    "        idx = np.argmax(spk_rec.sum(dim=0).detach().numpy())\n",
    "        if idx == test_targets[i]:\n",
    "            #print(f'match for {test_targets[i]}')\n",
    "            acc += 1\n",
    "        else:\n",
    "            #print(f'Not match for {test_targets[i]}')\n",
    "            pass\n",
    "    # print(f'Accuracy of the model : {acc}%')\n",
    "    logger.info(f'Accuracy of the model : {acc}%')\n",
    "\n",
    "# print()\n",
    "logger.info(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$1 \\le t \\le num\\_steps$$\n",
    "$$0 \\le j \\le num\\_layers-1$$\n",
    "$$0 \\le i \\le num\\_nodes-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMT encoding\n",
    "\n",
    "# take a random input and make it into a spike train\n",
    "layers = [num_input, num_hidden, num_output]\n",
    "spike_indicators = {}\n",
    "for t in range(num_steps):\n",
    "    for j, m in enumerate(layers):\n",
    "        for i in range(m):\n",
    "            spike_indicators[(i, j, t+1)] = Bool(f'x_{i}_{j}_{t+1}')\n",
    "\n",
    "potentials = {}\n",
    "for t in range(num_steps+1):\n",
    "    for j, m in enumerate(layers):\n",
    "        if j == 0:\n",
    "            continue\n",
    "        for i in range(m):\n",
    "            potentials[(i, j, t)] = Real(f'P_{i}_{j}_{t}')\n",
    "\n",
    "weights = defaultdict(float)\n",
    "w1 = net.fc1.weight\n",
    "for j in range(len(w1)):\n",
    "    for i in range(len(w1[j])):\n",
    "        weights[(i, j, 0)] = float(w1[j][i])\n",
    "w2 = net.fc2.weight\n",
    "for j in range(len(w2)):\n",
    "    for i in range(len(w2[j])):\n",
    "        weights[(i, j, 1)] = float(w2[j][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon_0(i,0) \\triangleq (P_{i,0}=0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([(0, 0, 0), (1, 0, 0), (2, 0, 0), (3, 0, 0), (0, 1, 0), (1, 1, 0), (2, 1, 0), (3, 1, 0), (0, 2, 0), (1, 2, 0), (2, 2, 0), (3, 2, 0), (0, 3, 0), (1, 3, 0), (2, 3, 0), (3, 3, 0), (0, 4, 0), (1, 4, 0), (2, 4, 0), (3, 4, 0), (0, 0, 1), (1, 0, 1), (2, 0, 1), (3, 0, 1), (4, 0, 1), (0, 1, 1), (1, 1, 1), (2, 1, 1), (3, 1, 1), (4, 1, 1), (0, 2, 1), (1, 2, 1), (2, 2, 1), (3, 2, 1), (4, 2, 1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================\n",
    "# Potential Initializations\n",
    "pot_init = []\n",
    "for j, m in enumerate(layers):\n",
    "    if j == 0:\n",
    "        continue\n",
    "    for i in range(m):\n",
    "        pot_init.append(potentials[(i, j, 0)] == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, S means:\n",
    "$$\\epsilon_1(i,t) \\triangleq \\left(S_{i,t}=\\sum_{j\\in inSynapse(N_i)}x_{j,t}\\cdot w_{j,i}\\right)$$\n",
    "but in this code, it has different meaning:\n",
    "$$S = P_{i,j,t-1} + \\sum_{i\\in Layer_{j-1}}x_{i,j-1,t}\\cdot w_{j-1,i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Inputs\n",
    "'''\n",
    "assign = []\n",
    "for i, spikes_t in enumerate(sample_spike):\n",
    "    for j, spike in enumerate(spikes_t):\n",
    "        if spike == 1:\n",
    "            assign.append(spike_indicators[(j, 0, i+1)])\n",
    "        else:\n",
    "            assign.append(Not(spike_indicators[(j, 0, i + 1)]))\n",
    "'''\n",
    "assign = []\n",
    "\n",
    "# Node eqn\n",
    "node_eqn = []\n",
    "\n",
    "for i in range(len(w1)):\n",
    "    node_eqn.append(\n",
    "        Implies(\n",
    "            Sum(\n",
    "                [\n",
    "                    If(weights[(k, i, 0)]>=0, weights[(k, i, 0)], 0) for k in range(len(w1[i]))\n",
    "                ]\n",
    "            ) < 1 * (1-0.95), # threshold * (1-lambda)\n",
    "            Not(Or(\n",
    "                [\n",
    "                    spike_indicators[(i, 1, t)] for t in range(1, num_steps+1)\n",
    "                ]\n",
    "            ))\n",
    "        )\n",
    "    )\n",
    "\n",
    "for i in range(len(w2)):\n",
    "    node_eqn.append(\n",
    "        Implies(\n",
    "            Sum(\n",
    "                [\n",
    "                    If(weights[(k, i, 1)]>=0, weights[(k, i, 1)], 0) for k in range(len(w2[i]))\n",
    "                ]\n",
    "            ) < 1 * (1-0.95), # threshold * (1-lambda)\n",
    "            Not(Or(\n",
    "                [\n",
    "                    spike_indicators[(i, 2, t)] for t in range(1, num_steps+1)\n",
    "                ]\n",
    "            ))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "for t in range(1, num_steps+1):\n",
    "    for j, m in enumerate(layers):\n",
    "        if j == 0:\n",
    "            continue\n",
    "\n",
    "        for i in range(m):\n",
    "            S = sum([spike_indicators[(k, j-1, t)]*weights[(k, i, j-1)] for k in range(layers[j-1])]) + potentials[(i, j, t-1)] # epsilon_1\n",
    "            node_eqn.append(\n",
    "                And(\n",
    "                    Implies(\n",
    "                        S >= 1.0,\n",
    "                        And(spike_indicators[(i, j, t)], potentials[(i, j, t)] == S - 1) # epsilon_2 & epsilon_4\n",
    "                    ),\n",
    "                    Implies(\n",
    "                        S < 1.0,\n",
    "                        And(Not(spike_indicators[(i, j, t)]), potentials[(i, j, t)] == beta*S) # epsilon_3 & epsilon_5\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            #print(f'==========================================================\\nAdded equation {(i,j,t)}')\n",
    "                \n",
    "\n",
    "#S.push()\n",
    "#print(\"Equations Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_samples = 15\n",
    "\n",
    "\n",
    "samples = iris_data[np.random.choice(range(len(iris_data)), num_samples)]\n",
    "# print(samples)\n",
    "logger.info(samples)\n",
    "deltas = [1, 2, 3]\n",
    "\n",
    "delta_v = {d: 0 for d in deltas}\n",
    "\n",
    "for delta in deltas:\n",
    "    avt = 0\n",
    "    for sample_no, sample in enumerate(samples):\n",
    "        sample_spike = spikegen.rate(torch.tensor(sample, dtype=torch.float), num_steps=num_steps)\n",
    "\n",
    "        spk_rec, mem_rec = net(sample_spike.view(num_steps, -1)) # epsilon 1~5\n",
    "        label = int(spk_rec.sum(dim=0).argmax())\n",
    "\n",
    "        S = Solver()\n",
    "        S.add(assign+node_eqn+pot_init)\n",
    "        # S.add(node_eqn + pot_init)\n",
    "\n",
    "        sum_val = []\n",
    "        for timestep, spike_train in enumerate(sample_spike):\n",
    "            for i, spike in enumerate(spike_train.view(num_input)):\n",
    "                if spike == 1:\n",
    "                    sum_val.append(If(spike_indicators[(i, 0, timestep + 1)], 0.0, 1.0))\n",
    "                else:\n",
    "                    sum_val.append(If(spike_indicators[(i, 0, timestep + 1)], 1.0, 0.0))\n",
    "        prop = [sum(sum_val) <= delta]\n",
    "        S.add(prop)\n",
    "        '''\n",
    "        s = [[] for i in range(num_steps)]\n",
    "        sv = [Int(f's_{i + 1}') for i in range(num_steps)]\n",
    "        prop = []\n",
    "        for timestep, spike_train in enumerate(sample_spike):\n",
    "            for i, spike in enumerate(spike_train.view(num_input)):\n",
    "                if spike == 1:\n",
    "                    s[timestep].append(If(spike_indicators[(i, 0, timestep + 1)], 0.0, 1.0))\n",
    "                else:\n",
    "                    s[timestep].append(If(spike_indicators[(i, 0, timestep + 1)], 1.0, 0.0))\n",
    "        prop = [sv[i] == sum(s[i]) for i in range(num_steps)]\n",
    "        prop.append(sum(sv) <= delta)\n",
    "        # print(prop[0])\n",
    "        #print(f\"Inputs Property Done in {time.time() - tx} sec\")\n",
    "        '''\n",
    "\n",
    "        # Output property\n",
    "        #tx = time.time()\n",
    "        op = []\n",
    "        intend_sum = sum([2 * spike_indicators[(label, 2, timestep + 1)] for timestep in range(num_steps)])\n",
    "        for t in range(num_output):\n",
    "            if t != op:\n",
    "                op.append(\n",
    "                    Not(intend_sum > sum([2 * spike_indicators[(t, 2, timestep + 1)] for timestep in range(num_steps)]))\n",
    "                )\n",
    "        #print(f'Output Property Done in {time.time() - tx} sec')\n",
    "        S.add(op)\n",
    "        tx = time.time()\n",
    "        res = S.check()\n",
    "        if str(res) == 'unsat':\n",
    "            delta_v[delta] += 1\n",
    "        else:\n",
    "            '''\n",
    "            sadv = np.zeros((num_steps, num_input), dtype=float)\n",
    "            m = S.model()\n",
    "            for tt in range(num_steps):\n",
    "                for k in range(num_input):\n",
    "                    sadv[tt][k] = 1 if str(m[spike_indicators[(k, 0, tt + 1)]]) == 'True' else 0\n",
    "            print()\n",
    "            '''\n",
    "            pass\n",
    "        del S\n",
    "        tss = time.time()-tx\n",
    "        # print(f'Completed for delta = {delta}, sample = {sample_no} in {tss} sec as {res}')\n",
    "        logger.info(f'Completed for delta = {delta}, sample = {sample_no} in {tss} sec as {res}')\n",
    "        avt = (avt*sample_no + tss)/(sample_no+1)\n",
    "    # print(f'Completed for delta = {delta} with {delta_v[delta]} in avg time {avt} sec')\n",
    "    logger.info(f'Completed for delta = {delta} with {delta_v[delta]} in avg time {avt} sec')\n",
    "\n",
    "\n",
    "'''\n",
    "m = S.model()\n",
    "for k in range(num_output):\n",
    "    names = []\n",
    "    for i in m.decls():\n",
    "        t = i.name().split('_')\n",
    "        if t[0] == 'x' and t[1] == f'{k}' and t[2] == '2':\n",
    "            names.append(i)\n",
    "    for i in sorted(names, key=functools.cmp_to_key(compare)):\n",
    "        print(f'{i}->{m[i]}')\n",
    "    input()\n",
    "    print()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [[0.84810127 0.70454545 0.63768116 0.56      ]\n",
    "#  [0.81012658 0.65909091 0.62318841 0.52      ]\n",
    "#  [0.59493671 0.72727273 0.23188406 0.08      ]\n",
    "#  [0.81012658 0.70454545 0.79710145 0.72      ]\n",
    "#  [0.73417722 0.90909091 0.17391304 0.08      ]]\n",
    "# Completed for delta = 1, sample = 0 in 14.885828018188477 sec as unsat\n",
    "# Completed for delta = 1, sample = 1 in 14.03385615348816 sec as unsat\n",
    "# Completed for delta = 1, sample = 2 in 8.778179407119751 sec as unsat\n",
    "# Completed for delta = 1, sample = 3 in 13.298641443252563 sec as unsat\n",
    "# Completed for delta = 1, sample = 4 in 2.512458562850952 sec as sat\n",
    "# Completed for delta = 1 with 4 in avg time 10.70179271697998 sec\n",
    "# Completed for delta = 2, sample = 0 in 182.78500318527222 sec as unsat\n",
    "# Completed for delta = 2, sample = 1 in 50.21033048629761 sec as unsat\n",
    "# Completed for delta = 2, sample = 2 in 1.3425533771514893 sec as sat\n",
    "# Completed for delta = 2, sample = 3 in 121.07727599143982 sec as unsat\n",
    "# Completed for delta = 2, sample = 4 in 3.9044606685638428 sec as sat\n",
    "# Completed for delta = 2 with 3 in avg time 71.863924741745 sec\n",
    "\n",
    "# [[0.84810127 0.75       0.82608696 0.84      ]\n",
    "#  [0.87341772 0.70454545 0.71014493 0.6       ]\n",
    "#  [0.62025316 0.68181818 0.20289855 0.08      ]\n",
    "#  [0.6835443  0.77272727 0.24637681 0.08      ]\n",
    "#  [0.84810127 0.75       0.82608696 0.84      ]]\n",
    "# Completed for delta = 1, sample = 0 in 13.993758916854858 sec as unsat\n",
    "# Completed for delta = 1, sample = 1 in 16.232289791107178 sec as unsat\n",
    "# Completed for delta = 1, sample = 2 in 3.728602647781372 sec as sat\n",
    "# Completed for delta = 1, sample = 3 in 3.0936505794525146 sec as sat\n",
    "# Completed for delta = 1, sample = 4 in 15.692162275314331 sec as unsat\n",
    "# Completed for delta = 1 with 3 in avg time 10.54809284210205 sec\n",
    "# Completed for delta = 2, sample = 0 in 206.85294008255005 sec as unsat\n",
    "# Completed for delta = 2, sample = 1 in 91.93414258956909 sec as unsat\n",
    "# Completed for delta = 2, sample = 2 in 5.711108207702637 sec as sat\n",
    "# Completed for delta = 2, sample = 3 in 3.172511577606201 sec as sat\n",
    "# Completed for delta = 2, sample = 4 in 117.94588279724121 sec as unsat\n",
    "# Completed for delta = 2 with 3 in avg time 85.12331705093384 sec\n",
    "\n",
    "# [[0.84810127 0.70454545 0.63768116 0.56      ]\n",
    "#  [0.64556962 0.86363636 0.23188406 0.08      ]\n",
    "#  [0.72151899 0.59090909 0.50724638 0.4       ]\n",
    "#  [0.7721519  0.68181818 0.71014493 0.72      ]\n",
    "#  [0.65822785 0.79545455 0.2173913  0.08      ]]\n",
    "# Completed for delta = 1, sample = 0 in 15.12482500076294 sec as unsat\n",
    "# Completed for delta = 1, sample = 1 in 2.7559990882873535 sec as sat\n",
    "# Completed for delta = 1, sample = 2 in 6.47345232963562 sec as unsat\n",
    "# Completed for delta = 1, sample = 3 in 9.045020818710327 sec as unsat\n",
    "# Completed for delta = 1, sample = 4 in 2.6043717861175537 sec as sat\n",
    "# Completed for delta = 1 with 3 in avg time 7.200733804702759 sec\n",
    "# Completed for delta = 2, sample = 0 in 43.941203355789185 sec as unsat\n",
    "# Completed for delta = 2, sample = 1 in 2.511519432067871 sec as sat\n",
    "# Completed for delta = 2, sample = 2 in 94.44714975357056 sec as unsat\n",
    "# Completed for delta = 2, sample = 3 in 153.38948512077332 sec as unsat\n",
    "# Completed for delta = 2, sample = 4 in 5.546740531921387 sec as sat\n",
    "# Completed for delta = 2 with 3 in avg time 59.96721963882446 sec\n",
    "\n",
    "# [[0.59493671 0.72727273 0.1884058  0.08      ]\n",
    "#  [0.72151899 0.65909091 0.60869565 0.52      ]\n",
    "#  [0.87341772 0.70454545 0.73913043 0.92      ]\n",
    "#  [0.84810127 0.56818182 0.84057971 0.72      ]\n",
    "#  [0.84810127 0.56818182 0.84057971 0.72      ]]\n",
    "# Completed for delta = 1, sample = 0 in 3.5639681816101074 sec as sat\n",
    "# Completed for delta = 1, sample = 1 in 13.699343204498291 sec as unsat\n",
    "# Completed for delta = 1, sample = 2 in 16.448461771011353 sec as unsat\n",
    "# Completed for delta = 1, sample = 3 in 14.851109743118286 sec as unsat\n",
    "# Completed for delta = 1, sample = 4 in 11.387865781784058 sec as unsat\n",
    "# Completed for delta = 1 with 4 in avg time 11.990149736404419 sec\n",
    "# Completed for delta = 2, sample = 0 in 4.561338901519775 sec as sat\n",
    "# Completed for delta = 2, sample = 1 in 121.3413896560669 sec as unsat\n",
    "# Completed for delta = 2, sample = 2 in 108.89543128013611 sec as unsat\n",
    "# Completed for delta = 2, sample = 3 in 121.82413840293884 sec as unsat\n",
    "# Completed for delta = 2, sample = 4 in 65.35597896575928 sec as unsat\n",
    "# Completed for delta = 2 with 4 in avg time 84.39565544128418 sec\n",
    "\n",
    "# [[0.59493671 0.72727273 0.23188406 0.08      ]\n",
    "#  [0.60759494 0.70454545 0.23188406 0.08      ]\n",
    "#  [0.55696203 0.65909091 0.20289855 0.08      ]\n",
    "#  [0.86075949 0.72727273 0.85507246 0.92      ]\n",
    "#  [0.82278481 0.63636364 0.66666667 0.6       ]]\n",
    "# Completed for delta = 1, sample = 0 in 12.557798862457275 sec as unsat\n",
    "# Completed for delta = 1, sample = 1 in 2.2979931831359863 sec as sat\n",
    "# Completed for delta = 1, sample = 2 in 7.2437708377838135 sec as sat\n",
    "# Completed for delta = 1, sample = 3 in 19.177116870880127 sec as unsat\n",
    "# Completed for delta = 1, sample = 4 in 11.036794185638428 sec as unsat\n",
    "# Completed for delta = 1 with 3 in avg time 10.462694787979126 sec\n",
    "# Completed for delta = 2, sample = 0 in 4.473829507827759 sec as sat\n",
    "# Completed for delta = 2, sample = 1 in 2.118896484375 sec as sat\n",
    "# Completed for delta = 2, sample = 2 in 5.217900276184082 sec as sat\n",
    "# Completed for delta = 2, sample = 3 in 125.89900875091553 sec as unsat\n",
    "# Completed for delta = 2, sample = 4 in 66.86198091506958 sec as unsat\n",
    "# Completed for delta = 2 with 2 in avg time 40.91432318687439 sec\n",
    "\n",
    "# 6min 33s ± 1min 26s per loop (mean ± std. dev. of 5 runs, 1 loop each)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
